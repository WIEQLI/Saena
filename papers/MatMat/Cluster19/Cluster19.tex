\documentclass[conference,10pt]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
 
% these are not included originally in the template
% -------------------------------------------------
\usepackage{lipsum}
\usepackage{amsopn}
\DeclareMathOperator{\diag}{diag}
\usepackage{booktabs}
\usepackage{float}

% Tools for mathematical typesetting
\usepackage{algpseudocode} 
% Typesetting algorithms
\usepackage{algorithm}

\newcommand{\mr}[1]{\mynote{Majid}{blue}{#1}}
\newcommand{\hs}[1]{\mynote{Hari}{olive}{#1}}

\newcommand{\bs}[1]{\ensuremath{\boldsymbol #1}}
\newcommand{\mynote}[3]{
	\textcolor{#2}{\fbox{\bfseries\sffamily\scriptsize#1}}
		{\small$\blacktriangleright$\textsf{\emph{#3}}$\blacktriangleleft$}
}

\input{macros}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% supress the following warning:
% PDF inclusion : multiple pdfs with page group included in a single page
\pdfsuppresswarningpagegroup=1

\newcommand{\mvec}{\textsc{matvec}}
\newcommand{\mm}{\textsc{GEMM}}
\newcommand{\recmm}{\textsc{RECURS\_GEMM}}
\newcommand{\spr}{\textsc{split\_by\_row}}
\newcommand{\spc}{\textsc{split\_by\_col}}
\newcommand{\nnzsz}{\textsc{nnz\_mat\_size}}

%\usepackage[top=0.75in, bottom=1.0in, left=0.625in, right=0.625in]{geometry}

%\usepackage{subcaption}
%\captionsetup{compatibility=false}

\usepackage{dblfloatfix}

\usepackage{balance}

% -------------------------------------------------


\begin{document}

\title{A Divide and Conquer Distributed Matrix-Matrix Multiplication\\
%{
%\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore %and should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}


\iffalse
\author{\IEEEauthorblockN{Majid Rasouli}
\IEEEauthorblockA{\textit{School of Computing} \\
\textit{University of Utah}\\
Salt Lake City, Utah, USA \\
rasouli@cs.utah.edu}
\and
\IEEEauthorblockN{Robert M. Kirby}
\IEEEauthorblockA{\textit{School of Computing} \\
\textit{University of Utah}\\
Salt Lake City, Utah, USA \\
kirby@sci.utah.edu}
\and
\IEEEauthorblockN{Hari Sundar}
\IEEEauthorblockA{\textit{School of Computing} \\
\textit{University of Utah}\\
Salt Lake City, Utah, USA \\
hari@cs.utah.edu}
}
\fi


\maketitle
\iffalse
\begin{abstract}

Matrix-matrix multiplication (\textsc{GEMM}) is a widely used linear algebra primitive common in scientific computing and data sciences. While several highly-tuned libraries and implementations exist, these typically target either sparse or dense matrices. The performance of these tuned implementations on unsupported types can be poor, and this is critical in cases where the structure of the computations is associated with varying degrees of sparsity. One such example is Algebraic Multigrid (AMG), a popular solver and preconditioner for large sparse linear systems.
%, especially the ones obtained from the discretization of elliptic operators.
%
%A key bottleneck for the scalability of the setup phase of AMG is the Matrix-matrix multiplication (MatMult) required for computing the sparse grid representation. While significant work has been done on optimizing matrix-matrix multiplication, in this work, we optimize it in the context of the matrix multiplications encountered in the setup phase of AMG. 
In this work, we present a new divide and conquer sparse \textsc{GEMM}, that is also highly performant and scalable when the matrix becomes very dense, as in the case of AMG matrix hierarchies. We combine this with an efficient communication pattern during distributed-memory \textsc{GEMM} to provide performance comparable to state-of-the-art sparse matrix libraries like PETSc. Additionally, the performance and scalability of our method surpass PETSc when the density of the matrix increases. We demonstrate the efficacy of our methods by comparing our \textsc{GEMM} with PETSc for different levels of sparsity. 
% We compare our solver with PETSc to demonstrate the performance improvements gained using our methods.

\end{abstract}

\begin{IEEEkeywords}
Matrix-Matrix Product, GEMM, Algebraic Multigrid, AMG, Linear Algebra, Iterative Solver, Preconditioner, Sparse, Dense
\end{IEEEkeywords}
\fi
\subsection{Analysis of The Recursive Function}
\label{sec:compare}

First, we compare \textit{Case 2} (vertical split) with \textit{Case 3} (horizontal split). Starting with the advantage of \textit{Case 2}, it consists of $2$ recursive calls, but \textit{Case 3} has $4$. So the cost of preparing for the recursive calls and also the cost of function calls themselves are half in \textit{Case 2}. 

Splitting vertically (\textit{Case 2}) has a property that makes it less scalable than \textit{Case 3}. Consider the simplest case, Figure \ref{fig:skinny1}, splitting $A$ vertically once and we assume this is done in serial. When $A1 \times B1$ and $A2 \times B2$ are computed, they should be added together to have the final result in $C$. This process, adding the duplicates, has at least three disadvantages. First, more entries are being stored, because of the duplicates of the nonzeros. Second, we need to perform the sorting on a multiple of number of nonzeros of $C$, to prepare for the duplicate addition step. And finally, adding the duplicates. Also note that in the worst case, the size of the result data before adding the duplicates would be: \textit{number of vertical splits} $\times$ \textit{nnz(C)}. \mr{add a figure to clarify this claim.} 

The other problem with \textit{Case 2} is that, the dense buffer needed to do $A1 \times B1$ is of size $m \times n$, which is the same as the total size of the result matrix $C$. The same is true for the second part of the multiplication, $A2 \times B2$. In other words, the dense buffer is of size \textit{row size of A} $\times$ \textit{column size of B}, so performing \textit{Case 2} does not help reaching the recursion threshold.
But, after each horizontal split, the sizes of submatrices get closer to the threshold in the factor of $4$, because of halving $A$ row-wise and halving $B$ column-wise.

As the last comparison point, we will compare the last step of the recursive function, which is extracting the nonzeros from the dense buffer.
By performing \textit{Case 3}, we can compute each block of the result matrix $C$ (see Figure \ref{fig:skinny2}) independently after calling each recursive call. So actually dense buffers are a partition of the result matrix $C$. But, for \textit{Case 2}, for each of the two recursive calls in Figure \ref{fig:skinny1} we need to extract nonzeros from two dense buffers, each as the size of the whole matrix $C$, which makes this case more expensive.

\begin{figure}[thb]
    %\centering
    \includegraphics[width=8.8cm,height=3.1cm]{./figures/skinny001.pdf}
    \caption{Multiplication by performing \textit{Case 2} once. At the end duplicates should be added together.}
    \label{fig:skinny1}
\end{figure}

\begin{figure}[thb]
    %\centering
    \includegraphics[width=8.8cm,height=3.1cm]{./figures/skinny002.pdf}
    \caption{Multiplication by performing \textit{Case 3} once. In this case, the dense buffers are independent and all together make a partition for the matrix $C$.}
    \label{fig:skinny2}
\end{figure}

Now we analyze the other step of the recursive function, \textit{Case 1}, which has $3$ parts:
\begin{enumerate}
 \item prepare data,
 \item perform the multiplication, and
 \item extract nonzeros from the dense buffer
\end{enumerate}

\mr{add more details about the above steps, especially the first one.}

The important factor in this part is how large the threshold is set. We have done two experiments, one with threshold set to $10M$ (Figure \ref{fig:strong001}) and the other one set to $10k$ (Figure \ref{fig:strong002}). In these experiments the matrix is chosen small (5041 dof) to avoid performing any splitting, to show how much a difference the threshold makes just to perform \textit{Case 1}. Extracting the nonzeros is the dominant part of this case, which is dependent on the size of the threshold.

For instance, on 256 processors to do the nonzero extraction during performing one matrix-matrix multiplication, it takes $0.47s$ when the threshold is set to $10M$ and $0.03s$ when the threshold is set to $10k$. Obviously the trade-off of having a smaller threshold is being required to split the submatrices into smaller blocks and the longer the recursive sequence, the more expensive the recursive calls part gets.

Figure \ref{fig:strong003} shows the total time to do one matrix-matrix product for the mentioned thresholds.

\newpage

\begin{figure}[htbp]
    %\centering
    \includegraphics[width=9cm,height=5.9cm]{./figures/strong001.png}
    \caption{Strong scaling of only performing \textit{Case 1} of matrix-matrix product on matrix $1365$ from the Florida Matrix Collection, when the threshold is set to $10M$.}
    \label{fig:strong001}
\end{figure}



\begin{figure}[htbp]
    %\centering
    \includegraphics[width=9cm,height=5.9cm]{./figures/strong002.png}
    \caption{Strong scaling of only performing \textit{Case 1} of matrix-matrix product on matrix $1365$ from the Florida Matrix Collection, when the threshold is set to $10k$.}
    \label{fig:strong002}
\end{figure}



\begin{figure}[htbp]
    %\centering
    \includegraphics[width=9cm,height=5.9cm]{./figures/strong003.png}
    \caption{Strong scaling of performing one matrix-matrix product on matrix $1365$ from the Florida Matrix Collection, when $2$ different thresholds.}
    \label{fig:strong003}
\end{figure}

\balance

%\input{intro}
%\input{methods}
%\input{results}

%\newpage
%\bibliographystyle{IEEEtran}
%\bibliography{ref}

\end{document}
