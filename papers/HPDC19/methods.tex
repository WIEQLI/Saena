\section{Methods}
\label{sec:methods}

In this section, we present our matrix-matrix multiplication. Then explain our modifications to the triple matrix multiplications that are part of the AMG setup process.

\subsection{Matrix-Matrix Multiplication}
\label{sec:matmult}

%\subsubsection{Computation}
%\label{sec:comp}
Here we assume that the data is available locally, so we discuss it as a serial implementation. We have implemented \mm~ in a recursive fashion. We split the matrices recursively in two ways: split by half based on the matrix size, split by half based on number of nonzeros. First we explain the algorithms performing splitting based on the matrix sizes.

The recursive function, \recmm, includes three cases:
\begin{enumerate}
 \item Case 1: Stop the recursion: perform the multiplication.
 \item Case 2: A is horizontal.
 \item Case 3: A is vertical.
\end{enumerate}

\subsubsection{Case 1}
\label{sec:case1}
At the start of the recursive function, the number of nonzero rows of A ($A\_nnz\_row$) and nonzero columns of B ($B\_nnz\_col$) are being computed. A threshold for $\nnzsz = A\_nnz\_row \times B\_nnz\_col$ is set (for our experiments we use $20M$ \mr{explain why.}).
We continue splitting the matrices until the threshold is reached. Then, we preform the multiplication. We have implemented two methods for this case: dense data structure and hashmap.

%First method uses a dense matrix.
%The matrices are ordered as column-major.
In the first method, a dense matrix of size $\nnzsz$ is initialized to $0$. \mr{explain better:}Each nonzero of $B$ is multiplied by its corresponding nonzero of $A$ and the result will be added to the corresponding index in the dense matrix. At the end, we go through the dense matrix and add the nonzeros to the final multiplication matrix.

When $\nnzsz$ gets larger, it becomes inefficient to traverse the whole dense matrix and check for nonzeros in the final stage. To solve this issue, we have implemented another method that uses hashmaps. The entry's index is the key and its value is the hashmap's value. When we want to add the multiplication of nonzeros of $A$ and $B$ to the hashmap, we check if the index exists in the map. If it exists the value is being added to the existing one's. Otherwise a new entry will be added to the hashmap.

In Figure~\ref{fig:lap60}, we compare the two methods for a 3D Poisson problem of size $216k$. For $0 \leq \nnzsz \leq 10M$, in $1M$ steps, we show how which method is faster than the other one. For the lower range the first method is better and for the higher range the second one.

\begin{figure}[tbh]
 \centering
 \Description{Description}
 \includegraphics[width=8.5cm,height=4cm]{./figures/lap60_range.pdf}
 \caption{Case 1: comparison between dense structure method with hashmap for a 3D Poisson problem of size $216k$. The plot shows the number of times each method was better than the other one in intervals of $1M$ for $\nnzsz$.}
 \label{fig:lap60}
\end{figure}

Figure~\ref{fig:eco} shows the same experiment for matrix ID $1882$ from SuiteSparse (Florida) Matrix Collection.

\begin{figure}[tbh]
 \centering
 \Description{Description}
 \includegraphics[width=8.5cm,height=4cm]{./figures/eco_range.pdf}
 \caption{Case 1: comparison between dense structure method with hashmap for matrix ID $1882$ from SuiteSparse (Florida) Matrix Collection. The plot shows the number of times each method was better than the other one in intervals of $1M$ for $\nnzsz$}
 \label{fig:eco}
\end{figure}

A combination of these two methods would give us the best of both. The dense method is being used for the lower range and the hashmap for the higher range. Figure ~\ref{fig:mix} compares the mixed method with the basic two methods.

\begin{figure}[tbh]
 \centering
 \Description{Description}
 \includegraphics[width=8.5cm,height=5cm]{./figures/mix.pdf}
 \caption{Comparison of three ways for Case 1: only using hashmap, only using the dense structure, and a mix of both of them.}
 \label{fig:mix}
\end{figure}


\subsubsection{Case 2}
\label{sec:case2}
When A is horizontal, i.e. its row size is less than or equal to its column size, we halve A by column based on its column size (Figure ~\ref{fig:case2}). Since row size of B equals column size of A, we halve B by row, so it will be a similar split to A, but horizontally. 

\begin{figure}[tbh]
 \centering
 \Description{Description}
 \includegraphics[width=6cm,height=2.7cm]{./figures/case2_001.pdf}
 \caption{}
 \label{fig:case2}
\end{figure}

\begin{algorithm}[H] 
  %\footnotesize
  \caption{Case 2: $C = \recmm2(A, B)$} \label{alg:case2} 
  \begin{algorithmic}[1]
    \Require $A$, $B$
    \Ensure  $C$
    \State $(A1, A2) = \spc(A)$
    \State $(B1, B2) = \spr(B)$
    \State $C \leftarrow \recmm(A1,B1)$
    \State $C \leftarrow \recmm(A2,B2)$
    %\State $C \leftarrow \textsc{mergesort}(C1, C2)$
  \end{algorithmic}
\end{algorithm}

\subsubsection{Case 3}
\label{sec:case3}
When A is vertical, i.e. its row size is greater than its column size, we halve A by row based on its row size and halve B by column (Figure ~\ref{fig:case3}). In contrary to the previous case, column size of B is not related to row size of A, so they are split independently.

\begin{figure}[tbh]
 \centering
 \Description{Description}
 \includegraphics[width=6cm,height=2.5cm]{./figures/case3_001.pdf}
 \caption{1}
 \label{fig:case3}
\end{figure}

\begin{algorithm}[H] 
  %\footnotesize
  \caption{Case 3: $C = \recmm3(A, B)$} \label{alg:case3} 
  \begin{algorithmic}[1]
    \Require $A$, $B$
    \Ensure  $C$
    \State $(A1, A2) = \spr(A)$
    \State $(B1, B2) = \spc(B)$
    \State $C \leftarrow \recmm(A1,B1)$
    \State $C \leftarrow \recmm(A2,B1)$
    \State $C \leftarrow \recmm(A1,B2)$
    \State $C \leftarrow \recmm(A2,B2)$
    %\State $\textsc{sort}(C)$
  \end{algorithmic}
\end{algorithm}

We have also implemented splitting based on the number of nonzeros. In \textit{Case 2}, we split $A$ in a way to have half of nonzeros in $A1$, and the other half in $A2$. The same split is used for $B$. In \textit{Case 3}, we do the same, but separately for both $A$ and $B$.

\subsection{AMG Matrix Multiplication}
\label{sec:amg}

\mr{Short summary of AMG setup. Explain given the fine matrix, how coarsening is normally done, and where the triple matmult shows up as part of the Galerkin approximation. What else is done during setup. Basically, highlight that the matmult is the dominant part, and the part that is complex in parallel.}

To solve a large linear system 
\begin{equation}
 Ax = b
\end{equation}
using an Algebraic Multigrid approach, we scale down the system, solve it directly, then scale back the solution. To scale down the system, three different categories of matrices are being created: coarse matrices ($As$), restriction matrices ($Rs$) and prolongation matrices ($Ps$). To study more about computing $Ps$ and $Rs$ you can refer to \mr{put some papers}. Here we focus how to compute $As$, assuming $Ps$ and $Rs$ are available. We explain how to compute the first coarse matrix, the next ones can be computed similarly.

% original text starts here 
To compute the coarse matrix $Ac$, a triple matrix multiplication should be done:
\begin{equation}
 Ac = R \times A \times P
\end{equation}
and $R = P^T$ in a Galerkin approach, which we use in our solver.

In the following, we explain how the communication is being done to compute the coarse matrix $Ac$, using some properties that are available in AMG.

Matrices are partitioned on multiple processors by row blocks (Figure~\ref{fig:partition}). Matrices $A$ and $P$ have the same number of rows and consequently are partitioned the same way. $R$ has less number of rows and has a different partition.

\begin{figure}[tbh]
 \centering
 \Description{Description}
 \includegraphics[width=8cm,height=3cm]{./figures/partition.pdf}
 \caption{1}
 \label{fig:partition}
\end{figure}

The triple matrix product is being done in two parts: first $A \times P$, then $R \times B$, in which $B = A \times P$, so performing matrix-matrix multiplications (\mm) twice.

\subsubsection{Part 1}

In this part, we explain how to compute $B := A \times P$. We assume the same partition of rows of $A$ on also its columns (red lines). For columns of $P$ we use the partition of rows of $R$ (blue lines in Figure~\ref{fig:part1b}).
We focus how to do \mm ~on processor $P1$. To compute entry $B(i, j)$, we need to multiply row $i$ of $A$ with column $j$ of $P$ and add them together (since we are working with sparse matrices, we only consider the nonzeros).

\begin{figure}[tbh]
 \centering
 \Description{Description}
 \includegraphics[width=5.5cm,height=3cm]{./figures/part1b.pdf}
 \caption{1}
 \label{fig:part1b}
\end{figure}

Column $j$ of $P$ is distributed between all the processors, so we need to communicate all nonzeros of that column and then perform $B_{ij} = \sum_{k} A_{ik} P_{kj}$. To avoid that, we can use the fact that $R$ is the transpose of $P$ and we already have it in the multigrid hierarchy. We note that column blocks of $P$ (e.g. $r0$ in Figure~\ref{fig:part1c}) are actually row blocks of $R$ transposed ($rt0$, which is transpose of $r0$).

\begin{figure}[tbh]
 \centering
 \Description{Description}
 \includegraphics[width=5.5cm,height=3cm]{./figures/part1c.pdf}
 \caption{1}
 \label{fig:part1c}
\end{figure}

Algorithm \ref{alg:part1} shows how we do it in an overlapped fashion. $B_{i}$ is the row block of matrix $B$ on processor $i$ and $B_{ik}$ is the sub-block result of multiplying $A_i$ with $rt_i$.

\begin{algorithm}[H] 
  %\footnotesize
  \caption{Part 1: $B_i = A_i \times P$} \label{alg:part1} 
  \begin{algorithmic}[1]
    \Require $A_i$, $R$
    \Ensure  $B_i$ (result of $A_i \times P$)
    \State $R1 \leftarrow$ transpose of $R_i$ (locally)
    \For{$k=myrank:myrank+nprocs$}
      \State $R2 \leftarrow\ Irecv(transpose\ of\ R block)\ from\ right\ neighbor$
      \State $Isend(R1)\ to\ left\ neighbor$
      \State $B_{ik} \leftarrow \recmm(A_i, R1)$ 
      \State $wait\ for\ Isend\ and\ Irecv\ to\ finish$
      \State $swap(R1,R2)$
    \EndFor
    \State locally sort $B_i$ and add duplicates
  \end{algorithmic}
\end{algorithm}


\subsubsection{Part 2}

Now we explain how to do the second \mm: $R \times B$. The blocks of $R$ on processor $P1$ in, Figure~\ref{fig:part2d}, should be multiplied with the corresponding blocks of $B$ with the same color. In contrary to the previous part, we already don't have the transpose of the right-hand side matrix and performing the transpose or communicating whole matrix $B$ between processors is expensive. Instead, we change the way the multiplication is being done and do a cheaper communication at the end.

\begin{figure}[tbh]
 \centering
 \Description{Description}
 \includegraphics[width=5.5cm,height=3cm]{./figures/part2d.pdf}
 \caption{1}
 \label{fig:part2d}
\end{figure}

In this case we have the transpose of the left-hand side matrix, so we use $P$ instead of $R$ to do this multiplication.
We only perform the multiplication of the green blocks on processor $P1$ in Figure~\ref{fig:part2e}, which can be done locally.

In the previous case each sub-block of the local left-hand side is multiplied by only one sub-block of the local right-hand side.
But in this method, each sub-block of local $P$ is being multiplied by whole row block of $B$, i.e. $PT10$ is multiplied by all $B10$, $B11$ and $B12$, and so on.

\begin{figure}[tbh]
 \centering
 \Description{Description}
 \includegraphics[width=5.5cm,height=3cm]{./figures/part2e.pdf}
 \caption{1}
 \label{fig:part2e}
\end{figure}

\mr{explain better}Using this method, we need to add duplicates twice (explain duplicates). Once, after doing the multiplication; Second time, after sorting the result globally and putting the entries on the correct processors (Algorithm~\ref{alg:part2}).

\begin{algorithm}[H] 
  %\footnotesize
  \caption{Part 2: $Ac = R \times B$} \label{alg:part2} 
  \begin{algorithmic}[1]
    \Require $P_i$, $B_i$
    \Ensure  $Ac_i$
    \State $PT_i \leftarrow$ transpose of $P_i$ (locally)
    \For{$k=0:nprocs$}
      \State $Ac_i \leftarrow \recmm(PT_{ik}, B_i)$
    \EndFor
    \State locally sort $Ac_i$ and add duplicates
    \State globally sort $Ac_i$ and add duplicates
  \end{algorithmic}
\end{algorithm}
