\section{Introduction}
\label{sec:intro}

The solution of elliptic partial differential equation (PDE) operators forms the 
foundation of the mathematical models of several engineering applications.
In solid mechanics, the stiffness matrix derived from linear elasticity represents an elliptic
operator that, when discretized with the finite element method (FEM) yields a symmetric positive
definite system. In fluid mechanics, the viscous terms and the pressure components of
the incompressible Navier-Stokes equations, when discretized, often lead to symmetric
positive definite systems. 
For the solution of such large-scale systems that need to be solved in parallel, iterative solvers 
with $\mathcal{O}(n)$ complexity and mesh-independent convergence are preferred. 
%
The most popular methods in this regard have been the multigrid methods--both geometric and algebraic. 
The geometric multigrid (GMG) methods when applied
to matrices generated from regular (often evenly-spaced) discretizations of elliptic operators \cite{MadayMunoz88,BrambleZhang00,Brenner02,GholamiMalhotraSundar2016}.
The mathematical predictability of the benefits of the GMG approach combined with the ability to
exploit the regularity of the data structures (in terms of indexing, coarsening, etc.) has made
GMG, especially in conjunction with preconditioned conjugate gradient (PCG) \cite{Braess86,TatebeOyanagi94}, the solver of choice when solving engineering applications that require large-scale
parallel solution approaches.  The story is more mixed when one moves to unstructured discretizations
and corresponding to algebraic multigrid (AMG), the communities alternative to GMG for such problems.  

While AMG is very attractive due to its black-box nature \cite{Dendy82,Vanek:1995,VanekBrezinaMandelEtAl01}, it does not scale as well as GMG \cite{Sundar12}. This is primarily due to
the loss of sparsity at coarser levels arising from the Galerkin approximation \cite{treister2015non}, leading to poor scalability, especially at the coarser levels.
Additionally, while both geometric and algebraic variants require an initial {\bf setup} phase, the cost of the setup is significantly higher for AMG, making it less attractive unless multiple solves are performed for the same operator. This is clearly unattractive for dynamic systems where the operator is changing rapidly, such as systems with $h$ or $p$ enrichment. In such cases, while the cost of an AMG solve might be lower than say that of CG, the overall cost i.e., setup + solve might be more than using asymptotically inferior solvers. In most cases, the scalbility of 
setup phase is also poor and typically worse than that of the solve phase. This is limited the attractiveness of AMG for large systems. In this work, we develop an efficient sparse matrix multiplication (\mm) algorithm to improve the performance and scalability of AMG. As will be explained in the following section, a sparse \mm\ is the dominant part of the AMG setup. While several of our optimizations apply to sparse \mm\ in general, and can be more broadly applied, our strategies for reducing inter-process communication make use of the special structure of \mm\ encountered during AMG setup.

\subsection{Background}
\label{sec:bg}

We start with a brief description of our AMG framework. 
AMG has been a popular method for solving the large-scale and 
often sparse linear system one obtains from discretization of elliptic partial 
differential equations.  The linear system can be written as
\begin{equation}
 Ax = b
\end{equation}
\noindent in which, $A \in R^{n \times n}$, $x$ and $b \in R^{n}$.
AMG consists of a setup and a solve phase.
%Three different sets of matrices will be created during the setup phase,
%and they will be used in the solve phase to solve the linear system.
The first step of the setup phase is to aggregate the nodes of the equivalent 
graph ($G$) of the matrix $A$. Every row of the matrix $A$ is considered as a node
in the graph $G$ and there is an edge between nodes $i$ and $j$ if entry $(i,j)$ is nonzero in $A$.
%This next sentence does not make sense - "By doing the aggregation on them??".  
After aggregation, %By doing the aggregation on them, 
some nodes will be chosen as \textit{roots} and the
rest of the nodes of the graph will be assigned to them.
Multiple aggregation methods for AMG have been introduced, such as
\cite{bell2012exposing}, \cite{notay2010aggregation},
\cite{Guillard98anaggregation}, %\cite{DBLP:journals/siamsc/SalaT08}, \cite{Vanek:1995}
\cite{DBLP:journals/siammax/Notay06}. %\cite{DBLP:journals/siamsc/Notay12}, \cite{BrezinaFMMMR05}, \cite{Brezina}
For this paper, \textit{maximal independent set} from \cite{bell2012exposing}, with some modifications, is used.

Given a linear system we have $n$ 
nodes in the graph $G$ and $m$ nodes are chosen as the \textit{roots}.
We compute prolongation $P$ and Restriction $R$ operators using the \textit{roots}. 
%A matrix, called the \textit{prolongation matrix} $P$, of size $n \times m$ is defined based on these nodes. If node $i$ is assigned to root $j$, then $P(i,j) = 1$, otherwise it is $0$. The restriction matrix $R \in R^{m \times n}$ is then made by transposing $P$.
The prolongation operator has two applications. It can interpolate a vector $v \in R^m$ to $v' \in R^n$, such that $m < n$.
The other application is creating a coarse version of the operator $A$ using the Galerkin approximation:

\begin{equation}
  \label{eq:rap}
 A_c = R \times A \times P
\end{equation}

\noindent such that $A_c \in R^{m \times m}$. This operation is called \textit{coarsening}.
The restriction operator is used similarly. The triple \mm\ in (\ref{eq:rap}) is the dominant cost of the AMG setup, especially when done in parallel. Improving the efficiency and scalability of this step in the main contribution of this work. 
%; in addition to being used in the coarsening, it takes a vector $w \in R^n$ to $R^m$.

Progressively coarser versions of the matrix are created during the setup phase corresponding to
a hierarchy (i.e. multi-level or ``multigrid") of data structures.
An AMG hierarchy of $L+1$ levels consists of three categories of operators:
coarse matrices ($As$), prolongation matrices ($Ps$) and restriction matrices ($Rs$).

%\begin{enumerate}
% \item \underline{Coarse Matrices}:       $As[0]$, $As[1]$, ..., $As[L]$, where $As[0]$ represents the finest matrix $A$
% \item \underline{Prolongation Matrices}: $Ps[0]$, $Ps[1]$, ..., $Ps[L-1]$
% \item \underline{Restriction Matrices}:  $Rs[0]$, $Rs[1]$, ..., $Rs[L-1]$
%\end{enumerate}

The coarse matrices for each level are created similar to $A_c$:
\begin{equation*}
 As[l+1] = Rs[l] \times As[l] \times Ps[l],\ l = 0, 1, ..., L
\end{equation*}
such that $As[0]$ is the finest matrix $A$. Again note that this is very expensive even at the coarser levels as the matrices get denser at the coarser levels. Our \mm\ maintains good performance and scalability across all levels of the multigrid hierarchy. 

The next phase of AMG is the solve phase. To solve $Ax = b$, we start with an initial guess for $x$.
% Two methods are utilized in the solve phase to find the approximate solution:
% \begin{enumerate}
%  \item Relaxation
%  \item Coarse-Grid Correction (CGC)
% \end{enumerate}
%
% Algorithm \ref{alg:amg} shows the AMG loop, which repeats until the solution is found within the 
% desired accuracy. The grid $g$ is the AMG hierarchy, consisting the three matrix categories mentioned previously.
% %of the coarse matrices ($As$), prolongation operators ($Ps$), and restriction operators ($Rs$).
%
% \begin{algorithm}[ht] 
%   \caption{AMG} \label{alg:amg} 
%   \begin{algorithmic}[1]
%     \Require $grid\ (g)$, $b$ and $initial\ guess\ (x_0)$
%     \Ensure  $solution\ (x)$
%     \While{not converged}
%       \State $x \leftarrow\ vcycle(g,\ x_0,\ b,\ 0$)
%     \EndWhile
%   \end{algorithmic}
% \end{algorithm}
The solution is computed in a recursive function \textit{vcycle} (Algorithm \ref{alg:vcycle}).
Regular smoothers are used in the relaxation part, such as Jacobi, Chebyshev, etc.
Then, the residual $r$ is computed. Next, $r$ is taken to the coarser level by using
the restriction operator ($R$).
The function recurses until it reaches the coarsest level ($L+1$). At that level,
the system will be solved directly. The solution of that system is actually the error,
which will be interpolated by $P$.
After that, the solution will be corrected by subtracting the interpolated error from it.
Finally, the solution will be smoothed again.

\begin{algorithm}[ht] 
  \caption{vcycle($g, x,\ b,\ l$)} \label{alg:vcycle} 
  \begin{algorithmic}[1]
    \Require $grid\ g$, $b$, $x$, and $level\ (l)$
    \Ensure  $solution\ (x)$
    \If{$l = L+1$}
      \State $x \leftarrow direct\ solver(g[L+1],\ x,\ b)$
    \Else
      \State $x \leftarrow Smoother(g, x,\ b,\ l)$
      \State $r \leftarrow As[l] \times x - b$
      \State $r_c \leftarrow Rs[l] \times r$
      \State $y_c \leftarrow$ $vcycle(g, x,\ r_c,\ l+1)$
      \State $y \leftarrow Ps[l] \times y_c$
      \State $x \leftarrow x - y$
      \State $x \leftarrow Smoother(g, x,\ b,\ l)$
    \EndIf
  \end{algorithmic}
\end{algorithm}

\textit{Smoothed Aggregation AMG (SA-AMG)}\cite{Vanek:1995} is a modified version of AMG,
in which the prolongation and restriction operators are smoothed %, for instance by the Jacobi method,
to improve the convergence of AMG.
For this paper, the improved version of SA-AMG in \cite{treister2015non} is used.


\subsection{Related Work}

While significant research has been done on improving the efficiency and scalability of sparse matrix-vector products, sparse \mm\ in comparison has received far less attention. Yuster and Zwick \cite{Yuster2005} provide a theoretically nearly optimal algorithm for multiplying sparse matrices, but rely on fast rectangular matrix multiplication. Consequently the approach is currently of only theoretical value. In \cite{Buluc12}, Buluc and Gilbert present algorithms for parallel sparse \mm\ using a two-dimensional block data distributions with serial hypersparse kernels. 
Gremse {\em et al.} \cite{Gremse15} the authors present a promising algorithm using iterative row merging to improve the performance on GPUs. Similarly, Saule {\em et al.} \cite{Saule14} evaluate the performance of sparse matrix multiplication kernels on the Intel Xeon Phi. Most AMG implementation have relied on standard sparse \mm\ implementations without any special considerations for the structure of the matrices generated within AMG. This work attempts to fill this gap. 

The main {\bf contributions} of this work are,
\begin{itemize}
  \item A new efficient \mm\ algorithm.
  \item A new method to communicate data required to do \mm~ specific to AMG
\end{itemize}

The rest of the paper is organized as follows. In \S\ref{sec:methods} we discuss the different strategies used to improve the performance and scalability of \mm. \hs{finish}

% \begin{itemize}
%   \item \st{General intro to AMG and elliptic operators, black box nature.}
%   \item \st{General outline of AMG method, setup and solve.}
%   \item \st{Why setup costs are important, what dominates}
%   \item what others have done.
%   \item This work and contributions.
%   \item rest of the paper.
% \end{itemize}
