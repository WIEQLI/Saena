\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{MadayMunoz88,BrambleZhang00,Brenner02,GholamiMalhotraSundar2016}
\citation{Braess86,TatebeOyanagi94}
\citation{Dendy82,Vanek:1995,VanekBrezinaMandelEtAl01}
\citation{Sundar12}
\citation{treister2015non}
\citation{bell2012exposing}
\citation{notay2010aggregation}
\citation{Guillard98anaggregation}
\citation{DBLP:journals/siammax/Notay06}
\citation{bell2012exposing}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Background}{1}{subsection.1.1}}
\newlabel{sec:bg}{{1.1}{1}{Background}{subsection.1.1}{}}
\citation{Vanek:1995}
\citation{treister2015non}
\citation{Yuster2005}
\citation{Buluc12}
\citation{Gremse15}
\citation{Saule14}
\newlabel{eq:rap}{{2}{2}{Background}{equation.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Related Work}{2}{subsection.1.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces vcycle($g, x,\ b,\ l$)\relax }}{2}{algorithm.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:vcycle}{{1}{2}{vcycle($g, x,\ b,\ l$)\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{2}{section.2}}
\newlabel{sec:methods}{{2}{2}{Methods}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Matrix-Matrix Multiplication}{2}{subsection.2.1}}
\newlabel{sec:matmult}{{2.1}{2}{Matrix-Matrix Multiplication}{subsection.2.1}{}}
\newlabel{sec:case1}{{2.1.1}{3}{Case 1}{subsubsection.2.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Case 1}{3}{subsubsection.2.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison between dense structure method with hashmap to compute coarse matrix $Ac = R \times A \times P$, in which $A$ is the 3D Poisson problem of size $216k$. The plot shows the number of times each method is faster than the other one in intervals of $1M$ for $\textsc  {nnz\_mat\_size}$.\relax }}{3}{figure.caption.4}}
\newlabel{fig:lap60}{{1}{3}{Comparison between dense structure method with hashmap to compute coarse matrix $Ac = R \times A \times P$, in which $A$ is the 3D Poisson problem of size $216k$. The plot shows the number of times each method is faster than the other one in intervals of $1M$ for $\nnzsz $.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison between dense structure method with hashmap to compute coarse matrix $Ac = R \times A \times P$, in which $A$ is matrix ID $1882$ from SuiteSparse (Florida) Matrix Collection. The plot shows the number of times each method is faster than the other one in intervals of $1M$ for $\textsc  {nnz\_mat\_size}$.\relax }}{3}{figure.caption.5}}
\newlabel{fig:eco}{{2}{3}{Comparison between dense structure method with hashmap to compute coarse matrix $Ac = R \times A \times P$, in which $A$ is matrix ID $1882$ from SuiteSparse (Florida) Matrix Collection. The plot shows the number of times each method is faster than the other one in intervals of $1M$ for $\nnzsz $.\relax }{figure.caption.5}{}}
\newlabel{sec:case2}{{2.1.2}{3}{Case 2}{subsubsection.2.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Case 2}{3}{subsubsection.2.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison between the three methods to do Case 1: only using hashmap, only using the dense structure, and the hybrid method. They are used as Case 1 to compute the first coarse matrix (the triple multiplication) on 7 matrices (3D Poisson) of different sizes.\relax }}{4}{figure.caption.6}}
\newlabel{fig:mix}{{3}{4}{Comparison between the three methods to do Case 1: only using hashmap, only using the dense structure, and the hybrid method. They are used as Case 1 to compute the first coarse matrix (the triple multiplication) on 7 matrices (3D Poisson) of different sizes.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Case 2: When A is horizontal, split A by column and B by row. Call the recursive function twice.\relax }}{4}{figure.caption.7}}
\newlabel{fig:case2_left}{{4}{4}{Case 2: When A is horizontal, split A by column and B by row. Call the recursive function twice.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Case 3: When A is vertical, split A by row and B by column. Call the recursive function four times.\relax }}{4}{figure.caption.8}}
\newlabel{fig:case3}{{5}{4}{Case 3: When A is vertical, split A by row and B by column. Call the recursive function four times.\relax }{figure.caption.8}{}}
\newlabel{sec:case3}{{2.1.3}{4}{Case 3}{subsubsection.2.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Case 3}{4}{subsubsection.2.1.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Case 2: $C = \textsc  {recurs\_matmult}2(A, B)$\relax }}{4}{algorithm.2}}
\newlabel{alg:case2}{{2}{4}{Case 2: $C = \recmm 2(A, B)$\relax }{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Case 3: $C = \textsc  {recurs\_matmult}3(A, B)$\relax }}{4}{algorithm.3}}
\newlabel{alg:case3}{{3}{4}{Case 3: $C = \recmm 3(A, B)$\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}All together}{4}{subsubsection.2.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}AMG Matrix-Multiplication: Communication}{4}{subsection.2.2}}
\newlabel{sec:amg}{{2.2}{4}{AMG Matrix-Multiplication: Communication}{subsection.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Partitioning of the matrices across the processors in row blocks. $A$ and $P$ have the same partition but $R$'s is different.\relax }}{5}{figure.caption.9}}
\newlabel{fig:partition}{{6}{5}{Partitioning of the matrices across the processors in row blocks. $A$ and $P$ have the same partition but $R$'s is different.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Part 1}{5}{subsubsection.2.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces This figure shows how the matrices are split into sub-blocks in Part 1. Column $j$ of $P$ is stored on different processors.\relax }}{5}{figure.caption.10}}
\newlabel{fig:part1b}{{7}{5}{This figure shows how the matrices are split into sub-blocks in Part 1. Column $j$ of $P$ is stored on different processors.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces This figure shows how row blocks of $R$ are transpose of column blocks of $P$.\relax }}{5}{figure.caption.11}}
\newlabel{fig:part1c}{{8}{5}{This figure shows how row blocks of $R$ are transpose of column blocks of $P$.\relax }{figure.caption.11}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Part 1: $B_i = A_i \times P$\relax }}{5}{algorithm.4}}
\newlabel{alg:part1}{{4}{5}{Part 1: $B_i = A_i \times P$\relax }{algorithm.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Part 2}{5}{subsubsection.2.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Part 2: $R \times B$. This figure shows which sub-blocks of R should be multiplied by which sub-blocks of $B$.\relax }}{5}{figure.caption.12}}
\newlabel{fig:part2d}{{9}{5}{Part 2: $R \times B$. This figure shows which sub-blocks of R should be multiplied by which sub-blocks of $B$.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Local transpose of blocks of $P$ is used instead of $R$. Each green sub-block of the transpose of $P$ is multiplied by all green sub-blocks of $B$.\relax }}{5}{figure.caption.13}}
\newlabel{fig:part2d}{{10}{5}{Local transpose of blocks of $P$ is used instead of $R$. Each green sub-block of the transpose of $P$ is multiplied by all green sub-blocks of $B$.\relax }{figure.caption.13}{}}
\citation{Sundar:2013}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Part 2: $Ac = R \times B$\relax }}{6}{algorithm.5}}
\newlabel{alg:part2}{{5}{6}{Part 2: $Ac = R \times B$\relax }{algorithm.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Numerical Results}{6}{section.3}}
\newlabel{sec:results}{{3}{6}{Numerical Results}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Left: Strong scaling for four banded matrices of the same size ($192k$), but with different bandwidth. The legend shows the density ($\frac  {nonzero}{size^2}$) of each matrix. Right: Comparison of the the strong scaling when using two different splitting strategies: based on matrix size and based on number of nonzeros. The legend shows the density of each matrix together with the splitting strategy.\relax }}{6}{figure.caption.14}}
\newlabel{fig:strong1}{{11}{6}{Left: Strong scaling for four banded matrices of the same size ($192k$), but with different bandwidth. The legend shows the density ($\frac {nonzero}{size^2}$) of each matrix. Right: Comparison of the the strong scaling when using two different splitting strategies: based on matrix size and based on number of nonzeros. The legend shows the density of each matrix together with the splitting strategy.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{6}{section.4}}
\newlabel{sec:conc}{{4}{6}{Conclusion}{section.4}{}}
\bibstyle{ACM-Reference-Format}
\bibdata{ref}
\bibcite{bell2012exposing}{{1}{2012}{{Bell et~al\unhbox \voidb@x \hbox {.}}}{{Bell, Dalton, and Olson}}}
\bibcite{Braess86}{{2}{1986}{{Braess}}{{Braess}}}
\bibcite{BrambleZhang00}{{3}{2000}{{Bramble and Zhang}}{{Bramble and Zhang}}}
\bibcite{Brenner02}{{4}{2002}{{Brenner}}{{Brenner}}}
\bibcite{Buluc12}{{5}{2012}{{Buluç and Gilbert}}{{Buluç and Gilbert}}}
\bibcite{Dendy82}{{6}{1982}{{Dendy}}{{Dendy}}}
\bibcite{GholamiMalhotraSundar2016}{{7}{2016}{{Gholami et~al\unhbox \voidb@x \hbox {.}}}{{Gholami, Malhotra, Sundar, and Biros}}}
\bibcite{Gremse15}{{8}{2015}{{Gremse et~al\unhbox \voidb@x \hbox {.}}}{{Gremse, Höfter, Schwen, Kiessling, and Naumann}}}
\bibcite{Guillard98anaggregation}{{9}{1998}{{Guillard and Vanek}}{{Guillard and Vanek}}}
\bibcite{MadayMunoz88}{{10}{1988}{{Maday and Mu{\~n}oz}}{{Maday and Mu{\~n}oz}}}
\bibcite{DBLP:journals/siammax/Notay06}{{11}{2006}{{Notay}}{{Notay}}}
\bibcite{notay2010aggregation}{{12}{2010}{{Notay}}{{Notay}}}
\bibcite{Saule14}{{13}{2014}{{Saule et~al\unhbox \voidb@x \hbox {.}}}{{Saule, Kaya, and {\c {C}}ataly{\"u}rek}}}
\bibcite{Sundar12}{{14}{2012}{{Sundar et~al\unhbox \voidb@x \hbox {.}}}{{Sundar, Biros, Burstedde, Rudi, Ghattas, and Stadler}}}
\bibcite{Sundar:2013}{{15}{2013}{{Sundar et~al\unhbox \voidb@x \hbox {.}}}{{Sundar, Malhotra, and Biros}}}
\bibcite{TatebeOyanagi94}{{16}{1994}{{Tatebe and Oyanagi}}{{Tatebe and Oyanagi}}}
\bibcite{treister2015non}{{17}{2015}{{Treister and Yavneh}}{{Treister and Yavneh}}}
\bibcite{VanekBrezinaMandelEtAl01}{{18}{2001}{{Van{\v {e}}k et~al\unhbox \voidb@x \hbox {.}}}{{Van{\v {e}}k, Brezina, Mandel, et~al\unhbox \voidb@x \hbox {.}}}}
\bibcite{Vanek:1995}{{19}{1995}{{Vanek et~al\unhbox \voidb@x \hbox {.}}}{{Vanek, Mandel, and Brezina}}}
\bibcite{Yuster2005}{{20}{2005}{{Yuster and Zwick}}{{Yuster and Zwick}}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{4.185pt}
\newlabel{tocindent2}{10.34999pt}
\newlabel{tocindent3}{18.198pt}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Comparison of the the strong scaling between our solver and PETSc. The legend shows the density of each matrix.\relax }}{7}{figure.caption.15}}
\newlabel{fig:petsc1}{{12}{7}{Comparison of the the strong scaling between our solver and PETSc. The legend shows the density of each matrix.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Weak scaling for two banded matrices: blue line shows the one with $24k$ on each node ($1k$ on each core) and red line shows a larger one with $100k$ on each node ($4166$ on each core).\relax }}{7}{figure.caption.16}}
\newlabel{fig:petsc1}{{13}{7}{Weak scaling for two banded matrices: blue line shows the one with $24k$ on each node ($1k$ on each core) and red line shows a larger one with $100k$ on each node ($4166$ on each core).\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{References}{7}{section*.18}}
\newlabel{TotPages}{{7}{7}{}{page.7}{}}
