\relax 
\citation{Dendy82}
\citation{azad2015parallel}
\citation{van2000graph}
\citation{gilbert2008unified}
\citation{kepner2011graph}
\citation{petsc-web-page,combblas}
\citation{ElemNewFrame}
\citation{petsc-web-page}
\citation{bienz2016reducing}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}\protected@file@percent }
\newlabel{sec:intro}{{I}{1}}
\citation{MadayMunoz88,BrambleZhang00,Brenner02,GholamiMalhotraSundar2016}
\citation{Braess86,TatebeOyanagi94}
\citation{Dendy82,Vanek:1995,VanekBrezinaMandelEtAl01}
\citation{Sundar12}
\citation{treister2015non}
\citation{bell2012exposing}
\citation{notay2010aggregation}
\citation{Guillard98anaggregation}
\citation{DBLP:journals/siammax/Notay06}
\citation{bell2012exposing}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces This figure shows an example of three levels of an AMG hierarchy (levels $3$, $4$ and $5$). The loss of sparsity is noticable from this figure.}}{2}\protected@file@percent }
\newlabel{fig:sparsity}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {I-A}}Background - AMG}{2}\protected@file@percent }
\newlabel{sec:bg}{{\unhbox \voidb@x \hbox {I-A}}{2}}
\citation{Vanek:1995}
\citation{treister2015non}
\citation{Yuster2005}
\citation{Buluc12}
\citation{Gremse15}
\citation{Saule14}
\newlabel{eq:rap}{{2}{3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces vcycle($g, x,\ b,\ l$)}}{3}\protected@file@percent }
\newlabel{alg:vcycle}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {I-B}}Related Work}{3}\protected@file@percent }
\newlabel{sec:related}{{\unhbox \voidb@x \hbox {I-B}}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Methods}{3}\protected@file@percent }
\newlabel{sec:methods}{{II}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Matrix-Matrix Multiplication}{3}\protected@file@percent }
\newlabel{sec:matmult}{{\unhbox \voidb@x \hbox {II-A}}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A basic scheme that shows splitting the matrix first horizontally, then vertically.}}{4}\protected@file@percent }
\newlabel{fig:split2}{{2}{4}}
\newlabel{sec:case1}{{\unhbox \voidb@x \hbox {II-A}1}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {II-A}1}Case 1}{4}\protected@file@percent }
\newlabel{eq:thres}{{4}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Using row and column sizes to decide when to stop the recursion is not efficient, because the top left block is the same size as the top right one, but it should be divided to more sub-blocks.}}{4}\protected@file@percent }
\newlabel{fig:thres}{{3}{4}}
\newlabel{eq:thres2}{{5}{4}}
\newlabel{sec:case2}{{\unhbox \voidb@x \hbox {II-A}2}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {II-A}2}Case 2}{5}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Case 2: $C = \textsc  {RECURS\_GEMM}2(A, B)$}}{5}\protected@file@percent }
\newlabel{alg:case2}{{2}{5}}
\newlabel{sec:case3}{{\unhbox \voidb@x \hbox {II-A}3}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {II-A}3}Case 3}{5}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Case 3: $C = \textsc  {RECURS\_GEMM}3(A, B)$}}{5}\protected@file@percent }
\newlabel{alg:case3}{{3}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {II-A}4}All together}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Communication}{5}\protected@file@percent }
\newlabel{sec:amg}{{\unhbox \voidb@x \hbox {II-B}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparison between dense structure method with hashmap to compute coarse matrix $Ac = R \times A \times P$, in which $A$ is the 3D Poisson problem of size $216k$. The plot shows the number of times each method is faster than the other one in intervals of $1M$ for $\textsc  {nnz\_mat\_size}$.}}{6}\protected@file@percent }
\newlabel{fig:lap60}{{4}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison between dense structure method with hashmap to compute coarse matrix $Ac = R \times A \times P$, in which $A$ is matrix ID $1882$ from SuiteSparse (Florida) Matrix Collection. The plot shows the number of times each method is faster than the other one in intervals of $1M$ for $\textsc  {nnz\_mat\_size}$.}}{6}\protected@file@percent }
\newlabel{fig:eco}{{5}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison between the three methods to do Case 1: only using hashmap, only using the dense structure, and the hybrid method. They are used in Case 1 part of \textsc  {RECURS\_GEMM}\nobreakspace  {} to compute the first coarse matrix (the triple multiplication) of 7 matrices (3D Poisson) of different sizes.}}{6}\protected@file@percent }
\newlabel{fig:mix}{{6}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Case 2: When A is horizontal, split A by column and B by row. Call the recursive function twice.}}{7}\protected@file@percent }
\newlabel{fig:case2_left}{{7}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Case 3: When A is vertical, split A by row and B by column. Call the recursive function four times.}}{7}\protected@file@percent }
\newlabel{fig:case3}{{8}{7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces $C_i = A_i \times B$}}{7}\protected@file@percent }
\newlabel{alg:comm1}{{4}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Partitioning of the matrices across the processors in row blocks.}}{7}\protected@file@percent }
\newlabel{fig:partition}{{9}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Column $j$ of $B$ is stored on different processors, so to compute entry $C_{ij}$ we need to multiply the parts of $A$ and $B$ with the same color.}}{7}\protected@file@percent }
\newlabel{fig:partition3}{{10}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces When $B$ is symmetric, we use local transpose of its row blocks.}}{7}\protected@file@percent }
\newlabel{fig:partition4}{{11}{7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces $C_i = A_i \times B$, when $B$ is symmetric.}}{7}\protected@file@percent }
\newlabel{alg:comm2}{{5}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Comparing splitting $B$ based on Algorithm\nobreakspace  {}4\hbox {} (middle) and Algorithm\nobreakspace  {}5\hbox {} (right)}}{8}\protected@file@percent }
\newlabel{fig:partition6}{{12}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Numerical Results}{8}\protected@file@percent }
\newlabel{sec:results}{{III}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Conclusion}{8}\protected@file@percent }
\newlabel{sec:conc}{{IV}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Weak scaling for two banded matrices: blue line shows the one with $24k$ on each node ($1k$ on each core) and red line shows a larger one with $100k$ on each node ($4166$ on each core).}}{8}\protected@file@percent }
\newlabel{fig:weak1}{{13}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Strong scaling for four banded matrices of the same size ($192k$), but with different bandwidth. The legend shows the density ($\frac  {nonzero}{size^2}$) of each matrix.}}{9}\protected@file@percent }
\newlabel{fig:strong2}{{14}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Comparison of the the strong scaling when using two different splitting strategies: based on matrix size and based on number of nonzeros. The legend shows the density of each matrix together with the splitting strategy. The solid and dash lines show the splitting based on nonzeros and matrix sizes, respectively. The lines for the cases with the same densities have the same marker (e.g. square) to be easily comparable.}}{9}\protected@file@percent }
\newlabel{fig:strongvs}{{15}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Comparison of the the strong scaling between our solver (solid lines) and PETSc (dash lines). The legend shows the density of each matrix. The lines for the cases with the same densities have the same marker (e.g. square) to be easily comparable.}}{9}\protected@file@percent }
\newlabel{fig:petsc1}{{16}{9}}
\bibstyle{IEEEtran}
\bibdata{ref}
\bibcite{Dendy82}{1}
\bibcite{azad2015parallel}{2}
\bibcite{van2000graph}{3}
\bibcite{gilbert2008unified}{4}
\bibcite{kepner2011graph}{5}
\bibcite{petsc-web-page}{6}
\bibcite{combblas}{7}
\bibcite{ElemNewFrame}{8}
\bibcite{bienz2016reducing}{9}
\bibcite{MadayMunoz88}{10}
\bibcite{BrambleZhang00}{11}
\bibcite{Brenner02}{12}
\bibcite{GholamiMalhotraSundar2016}{13}
\bibcite{Braess86}{14}
\bibcite{TatebeOyanagi94}{15}
\bibcite{Vanek:1995}{16}
\bibcite{VanekBrezinaMandelEtAl01}{17}
\bibcite{Sundar12}{18}
\bibcite{treister2015non}{19}
\bibcite{bell2012exposing}{20}
\bibcite{notay2010aggregation}{21}
\bibcite{Guillard98anaggregation}{22}
\bibcite{DBLP:journals/siammax/Notay06}{23}
\bibcite{Yuster2005}{24}
\bibcite{Buluc12}{25}
\bibcite{Gremse15}{26}
\bibcite{Saule14}{27}
\@writefile{toc}{\contentsline {section}{References}{10}\protected@file@percent }
