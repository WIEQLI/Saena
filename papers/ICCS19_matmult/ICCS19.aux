\relax 
\citation{MadayMunoz88,BrambleZhang00,Brenner02,GholamiMalhotraSundar2016}
\citation{Braess86,TatebeOyanagi94}
\@writefile{toc}{\contentsline {title}{Lazy-update Multigrid Preconditioners}{1}}
\@writefile{toc}{\authcount {3}}
\@writefile{toc}{\contentsline {author}{Majid Rasouli \and Robert M. Kirby \and Hari Sundar }{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{sec:intro}{{1}{1}}
\citation{Dendy82,Vanek:1995,VanekBrezinaMandelEtAl01}
\citation{Sundar12}
\citation{treister2015non}
\citation{bell2012exposing}
\citation{notay2010aggregation}
\citation{Guillard98anaggregation}
\citation{DBLP:journals/siammax/Notay06}
\citation{bell2012exposing}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Background}{2}}
\newlabel{sec:bg}{{1.1}{2}}
\citation{Vanek:1995}
\citation{treister2015non}
\newlabel{eq:rap}{{2}{3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces vcycle($g, x,\ b,\ l$)}}{3}}
\newlabel{alg:vcycle}{{1}{3}}
\citation{Yuster2005}
\citation{Buluc12}
\citation{Gremse15}
\citation{Saule14}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Related Work}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{4}}
\newlabel{sec:methods}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Matrix-Matrix Multiplication}{4}}
\newlabel{sec:matmult}{{2.1}{4}}
\newlabel{sec:case1}{{2.1}{5}}
\@writefile{toc}{\contentsline {subsubsection}{Case 1}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison between dense structure method with hashmap to compute coarse matrix $Ac = R \times A \times P$, in which $A$ is the 3D Poisson problem of size $216k$. The plot shows the number of times each method is faster than the other one in intervals of $1M$ for $\textsc  {nnz\_mat\_size}$.}}{6}}
\newlabel{fig:lap60}{{1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison between dense structure method with hashmap to compute coarse matrix $Ac = R \times A \times P$, in which $A$ is matrix ID $1882$ from SuiteSparse (Florida) Matrix Collection. The plot shows the number of times each method is faster than the other one in intervals of $1M$ for $\textsc  {nnz\_mat\_size}$.}}{6}}
\newlabel{fig:eco}{{2}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison between the three methods to do Case 1: only using hashmap, only using the dense structure, and the hybrid method. They are used as Case 1 to compute the first coarse matrix (the triple multiplication) on 7 matrices (3D Poisson) of different sizes.}}{7}}
\newlabel{fig:mix}{{3}{7}}
\newlabel{sec:case2}{{2.1}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Case 2}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Case 2: When A is horizontal, split A by column and B by row. Call the recursive function twice.}}{8}}
\newlabel{fig:case2}{{4}{8}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Case 2: $C = \textsc  {recurs\_matmult}2(A, B)$}}{8}}
\newlabel{alg:case2}{{2}{8}}
\newlabel{sec:case3}{{2.1}{8}}
\@writefile{toc}{\contentsline {subsubsection}{Case 3}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Case 3: When A is vertical, split A by row and B by column. Call the recursive function four times.}}{8}}
\newlabel{fig:case3}{{5}{8}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Case 3: $C = \textsc  {recurs\_matmult}3(A, B)$}}{9}}
\newlabel{alg:case3}{{3}{9}}
\@writefile{toc}{\contentsline {subsubsection}{All together}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}AMG Matrix-Multiplication: Communication}{9}}
\newlabel{sec:amg}{{2.2}{9}}
\@writefile{toc}{\contentsline {subsubsection}{Part 1}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Partitioning of the matrices across the processors in row blocks. $A$ and $P$ have the same partition but $R$'s is different.}}{10}}
\newlabel{fig:partition}{{6}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces This figure shows how the matrices are split into sub-blocks in Part 1. Column $j$ of $P$ is stored on different processors.}}{10}}
\newlabel{fig:part1b}{{7}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces This figure shows how row block of $R$ are transpose of column blocks of $P$.}}{11}}
\newlabel{fig:part1c}{{8}{11}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Part 1: $B_i = A_i \times P$}}{11}}
\newlabel{alg:part1}{{4}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Part 2}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Part 2: $R \times B$. This figure shows which sub-blocks of R should be multiplied by which sub-blocks of $B$.}}{11}}
\newlabel{fig:part2d}{{9}{11}}
\citation{Sundar:2013}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces 1}}{12}}
\newlabel{fig:part2e}{{10}{12}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Part 2: $Ac = R \times B$}}{12}}
\newlabel{alg:part2}{{5}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Numerical Results}{12}}
\newlabel{sec:results}{{3}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Strong scaling for five banded matrices of the same size (192k), but with different bandwidth. The legend shows the density ($\frac  {nonzero}{size^2}$) of each matrix.}}{13}}
\newlabel{fig:strong1}{{11}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Comparison of the the strong scaling when using two different splitting strategies: based on matrix size and based on number of nonzeros. The legend shows the density of each matrix.}}{13}}
\newlabel{fig:size_vs_nnz}{{12}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Comparison of the the strong scaling between our solver and PETSc. The number for each line shows the density for that matrix. The legend shows the density of each matrix.}}{14}}
\newlabel{fig:petsc1}{{13}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Weak scaling for two banded matrices: blue line shows the one with $24k$ on each node ($1k$ on each core) and red line shows the larger one with $100k$ on each node ($4166$ on each core)}}{14}}
\newlabel{fig:weak1}{{14}{14}}
\bibstyle{splncs04}
\bibdata{ref.bib}
\bibcite{bell2012exposing}{1}
\bibcite{Braess86}{2}
\bibcite{BrambleZhang00}{3}
\bibcite{Brenner02}{4}
\bibcite{Buluc12}{5}
\bibcite{Dendy82}{6}
\bibcite{GholamiMalhotraSundar2016}{7}
\bibcite{Gremse15}{8}
\bibcite{Guillard98anaggregation}{9}
\bibcite{MadayMunoz88}{10}
\bibcite{DBLP:journals/siammax/Notay06}{11}
\bibcite{notay2010aggregation}{12}
\bibcite{Saule14}{13}
\bibcite{Sundar12}{14}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{15}}
\newlabel{sec:conc}{{4}{15}}
\bibcite{Sundar:2013}{15}
\bibcite{TatebeOyanagi94}{16}
\bibcite{treister2015non}{17}
\bibcite{VanekBrezinaMandelEtAl01}{18}
\bibcite{Vanek:1995}{19}
\bibcite{Yuster2005}{20}
